{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EE 467 Lab 2: Breaking CAPTCHAs with PyTorch\n",
                "\n",
                "This notebook is the **PyTorch implementation** of the CAPTCHA breaking lab, converted from the original TensorFlow/Keras version.\n",
                "\n",
                "## Overview\n",
                "- Load and preprocess CAPTCHA images\n",
                "- Extract individual characters from CAPTCHAs\n",
                "- Build a CNN model using PyTorch\n",
                "- Train the model to recognize characters\n",
                "- Evaluate the full CAPTCHA recognition pipeline\n",
                "\n",
                "**Note:** The results should be similar to the TensorFlow version, with minor differences due to different random initialization and implementation details."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install matplotlib scikit-learn \"opencv-python>4\" imutils torch torchvision"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pickle\n",
                "import glob\n",
                "import math\n",
                "from pprint import pprint\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "import imutils\n",
                "from imutils import paths\n",
                "from matplotlib import pyplot as plt\n",
                "from matplotlib.gridspec import GridSpec\n",
                "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# PyTorch imports\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "\n",
                "# Import helper functions\n",
                "from lab_2_helpers import *\n",
                "\n",
                "# Check if GPU is available\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Preprocessing\n",
                "\n",
                "## Ground Truth Characters Extraction\n",
                "\n",
                "Load CAPTCHA images and extract labels from filenames."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset images folder\n",
                "CAPTCHA_IMAGE_FOLDER = \"./captcha-images\"\n",
                "\n",
                "# List of all the captcha images we need to process\n",
                "captcha_image_paths = list(paths.list_images(CAPTCHA_IMAGE_FOLDER))\n",
                "# Review image paths\n",
                "print(f\"Total CAPTCHA images: {len(captcha_image_paths)}\")\n",
                "pprint(captcha_image_paths[:10])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_captcha_text(image_path):\n",
                "    \"\"\" Extract correct CAPTCHA texts from file name of images. \"\"\"\n",
                "    # Extract file name of image from its path\n",
                "    # e.g. \"./captcha-images/2A2X.png\" -> \"2A2X.png\"\n",
                "    image_file_name = os.path.basename(image_path)\n",
                "    # Extract base name of image, omitting file extension\n",
                "    # e.g. \"2A2X.png\" -> \"2A2X\"\n",
                "    return os.path.splitext(image_file_name)[0]\n",
                "\n",
                "captcha_texts = [extract_captcha_text(image_path) for image_path in captcha_image_paths]\n",
                "# Review extraction results\n",
                "pprint(captcha_texts[:10])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading and Transforming Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_transform_image(image_path):\n",
                "    \"\"\" Load and transform image into grayscale with padding. \"\"\"\n",
                "    # 1) Load image with OpenCV\n",
                "    image = cv2.imread(image_path)\n",
                "    \n",
                "    # 2) Convert image to grayscale\n",
                "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
                "    \n",
                "    # 3) Add extra padding (8px) around the image\n",
                "    image_padded = cv2.copyMakeBorder(\n",
                "        image_gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE\n",
                "    )\n",
                "    \n",
                "    return image_padded\n",
                "\n",
                "captcha_images = [load_transform_image(image_path) for image_path in captcha_image_paths]\n",
                "\n",
                "# Review loaded CAPTCHAs\n",
                "print_images(\n",
                "    captcha_images[:10], n_rows=2, texts=captcha_texts[:10]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train-Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train-validation-test split seed\n",
                "TVT_SPLIT_SEED = 31528476\n",
                "\n",
                "# Perform split on CAPTCHA images as well as labels\n",
                "captcha_images_tv, captcha_images_test, captcha_texts_tv, captcha_texts_test = train_test_split(\n",
                "    captcha_images, captcha_texts, test_size=0.2, random_state=TVT_SPLIT_SEED\n",
                ")\n",
                "\n",
                "print(\"Train-validation:\", len(captcha_texts_tv))\n",
                "print(\"Test:\", len(captcha_texts_test))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bounding Box Extraction\n",
                "\n",
                "Extract individual characters from each CAPTCHA image using contour detection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Character images folder template\n",
                "CHAR_IMAGE_FOLDER = f\"./char-images-{TVT_SPLIT_SEED}\"\n",
                "\n",
                "def extract_chars(image):\n",
                "    \"\"\" Find contours and extract characters inside each CAPTCHA. \"\"\"\n",
                "    # Threshold image and convert it to black-white\n",
                "    image_bw = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
                "    # Find contours (continuous blobs of pixels) the image\n",
                "    contours = cv2.findContours(image_bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
                "\n",
                "    char_regions = []\n",
                "    # Loop through each contour\n",
                "    for contour in contours:\n",
                "        # Get the rectangle that contains the contour\n",
                "        x, y, w, h = cv2.boundingRect(contour)\n",
                "\n",
                "        # Compare the width and height of the bounding box,\n",
                "        # detect if there are letters conjoined into one chunk\n",
                "        if w / h > 1.25:\n",
                "            # Bounding box is too wide for a single character\n",
                "            # Split it in half into two letter regions\n",
                "            half_width = int(w / 2)\n",
                "            char_regions.append((x, y, half_width, h))\n",
                "            char_regions.append((x + half_width, y, half_width, h))\n",
                "        else:\n",
                "            # Only a single letter in contour\n",
                "            char_regions.append((x, y, w, h))\n",
                "\n",
                "    # Ignore image if less or more than 4 regions detected\n",
                "    if len(char_regions) != 4:\n",
                "        return None\n",
                "    # Sort regions by their X coordinates\n",
                "    char_regions.sort(key=lambda x: x[0])\n",
                "\n",
                "    # Character images\n",
                "    char_images = []\n",
                "    # Save each character as a single image\n",
                "    for x, y, w, h in char_regions:\n",
                "        # Extract character from image with 2px margin\n",
                "        char_image = image[y - 2:y + h + 2, x - 2:x + w + 2]\n",
                "        # Save character images\n",
                "        char_images.append(char_image)\n",
                "\n",
                "    # Return character images\n",
                "    return char_images\n",
                "\n",
                "def save_chars(char_images, captcha_text, save_dir, char_counts):\n",
                "    \"\"\" Save character images to directory. \"\"\"\n",
                "    for char_image, char in zip(char_images, captcha_text):\n",
                "        # Get the folder to save the image in\n",
                "        save_path = os.path.join(save_dir, char)\n",
                "        os.makedirs(save_path, exist_ok=True)\n",
                "\n",
                "        # Write letter image to file\n",
                "        char_count = char_counts.get(char, 1)\n",
                "        char_image_path = os.path.join(save_path, f\"{char_count}.png\")\n",
                "        cv2.imwrite(char_image_path, char_image)\n",
                "\n",
                "        # Update count\n",
                "        char_counts[char] = char_count + 1\n",
                "\n",
                "# Force character extraction even if results are already available\n",
                "FORCE_EXTRACT_CHAR = False\n",
                "\n",
                "char_counts = {}\n",
                "# Extract and save images for characters\n",
                "if FORCE_EXTRACT_CHAR or not os.path.exists(CHAR_IMAGE_FOLDER):\n",
                "    for captcha_image, captcha_text in zip(captcha_images_tv, captcha_texts_tv):\n",
                "        # Extract character images\n",
                "        char_images = extract_chars(captcha_image)\n",
                "        # Skip if extraction failed\n",
                "        if char_images is None:\n",
                "            continue\n",
                "        # Save character images\n",
                "        save_chars(char_images, captcha_text, CHAR_IMAGE_FOLDER, char_counts)\n",
                "    print(f\"Extracted characters saved to {CHAR_IMAGE_FOLDER}\")\n",
                "else:\n",
                "    print(f\"Using existing character images from {CHAR_IMAGE_FOLDER}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Label Encoding and Feature Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path of occurred characters (labels)\n",
                "LABELS_PATH = \"./labels_pytorch.pkl\"\n",
                "\n",
                "def make_feature(image):\n",
                "    \"\"\" Process character image and turn it into feature. \"\"\"\n",
                "    # Resize letter to 20*20\n",
                "    image_resized = resize_to_fit(image, 20, 20)\n",
                "    # Add extra dimension as the only channel\n",
                "    feature = image_resized[..., None]\n",
                "    return feature\n",
                "\n",
                "def make_feature_label(image_path):\n",
                "    \"\"\" Load character image and make feature-label pair from image path. \"\"\"\n",
                "    # Load image and make feature\n",
                "    feature = make_feature(cv2.imread(image_path, cv2.COLOR_BGR2GRAY))\n",
                "    # Extract label based on the directory the image is in\n",
                "    label = image_path.split(os.path.sep)[-2]\n",
                "    return feature, label\n",
                "\n",
                "# Make features and labels from character image paths\n",
                "features_tv, labels_tv = unzip((\n",
                "    make_feature_label(image_path) for image_path in paths.list_images(CHAR_IMAGE_FOLDER)\n",
                "))\n",
                "\n",
                "# Scale raw pixel values into range [0, 1]\n",
                "features_tv = np.array(features_tv, dtype=\"float\") / 255\n",
                "\n",
                "# For PyTorch, we use LabelEncoder for integer labels (CrossEntropyLoss expects class indices)\n",
                "le = LabelEncoder()\n",
                "labels_encoded = le.fit_transform(labels_tv)\n",
                "\n",
                "# Number of classes\n",
                "n_classes = len(le.classes_)\n",
                "print(f\"Number of classes: {n_classes}\")\n",
                "print(f\"Classes: {le.classes_}\")\n",
                "\n",
                "# Further split the training data into training and validation set\n",
                "X_train, X_vali, y_train, y_vali = train_test_split(\n",
                "    features_tv, labels_encoded, test_size=0.25, random_state=955996\n",
                ")\n",
                "\n",
                "# Save mapping from labels to encoding\n",
                "with open(LABELS_PATH, \"wb\") as f:\n",
                "    pickle.dump(le, f)\n",
                "\n",
                "print(f\"Training samples: {len(X_train)}\")\n",
                "print(f\"Validation samples: {len(X_vali)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare PyTorch DataLoaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert to PyTorch tensors\n",
                "# PyTorch expects (N, C, H, W) format, so we need to transpose from (N, H, W, C)\n",
                "X_train_tensor = torch.FloatTensor(X_train).permute(0, 3, 1, 2)  # (N, 1, 20, 20)\n",
                "X_vali_tensor = torch.FloatTensor(X_vali).permute(0, 3, 1, 2)\n",
                "y_train_tensor = torch.LongTensor(y_train)\n",
                "y_vali_tensor = torch.LongTensor(y_vali)\n",
                "\n",
                "# Create datasets\n",
                "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
                "vali_dataset = TensorDataset(X_vali_tensor, y_vali_tensor)\n",
                "\n",
                "# Batch size\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "# Create dataloaders\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "vali_loader = DataLoader(vali_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f\"Training batches: {len(train_loader)}\")\n",
                "print(f\"Validation batches: {len(vali_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training\n",
                "\n",
                "## Define the CNN Model in PyTorch\n",
                "\n",
                "The architecture matches the TensorFlow version:\n",
                "\n",
                "| Layer | Output Shape | Parameters |\n",
                "|-------|--------------|------------|\n",
                "| Input | (1, 20, 20) | - |\n",
                "| Conv2D(20, 5x5, same) + ReLU | (20, 20, 20) | 520 |\n",
                "| MaxPool2D(2x2) | (20, 10, 10) | - |\n",
                "| Conv2D(50, 5x5, same) + ReLU | (50, 10, 10) | 25,050 |\n",
                "| MaxPool2D(2x2) | (50, 5, 5) | - |\n",
                "| Flatten | (1250,) | - |\n",
                "| Dense(500) + ReLU | (500,) | 625,500 |\n",
                "| Dense(n_classes) + Softmax | (n_classes,) | varies |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CaptchaCNN(nn.Module):\n",
                "    \"\"\"\n",
                "    CNN for CAPTCHA character classification.\n",
                "    \n",
                "    Architecture:\n",
                "    - 2 Convolutional blocks (Conv2D + ReLU + MaxPool)\n",
                "    - Flatten\n",
                "    - 2 Fully connected layers\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, n_classes):\n",
                "        super(CaptchaCNN, self).__init__()\n",
                "        \n",
                "        # First convolution block\n",
                "        # Input: (batch, 1, 20, 20) -> Output: (batch, 20, 10, 10)\n",
                "        self.conv1 = nn.Conv2d(\n",
                "            in_channels=1,\n",
                "            out_channels=20,\n",
                "            kernel_size=5,\n",
                "            stride=1,\n",
                "            padding=2  # 'same' padding: (kernel_size - 1) / 2 = (5-1)/2 = 2\n",
                "        )\n",
                "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        \n",
                "        # Second convolution block\n",
                "        # Input: (batch, 20, 10, 10) -> Output: (batch, 50, 5, 5)\n",
                "        self.conv2 = nn.Conv2d(\n",
                "            in_channels=20,\n",
                "            out_channels=50,\n",
                "            kernel_size=5,\n",
                "            stride=1,\n",
                "            padding=2  # 'same' padding\n",
                "        )\n",
                "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        \n",
                "        # Fully connected layers\n",
                "        # After pooling: 50 channels * 5 * 5 = 1250\n",
                "        self.fc1 = nn.Linear(50 * 5 * 5, 500)\n",
                "        self.fc2 = nn.Linear(500, n_classes)\n",
                "        \n",
                "        # Activation function\n",
                "        self.relu = nn.ReLU()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # First conv block\n",
                "        x = self.conv1(x)           # (batch, 20, 20, 20)\n",
                "        x = self.relu(x)\n",
                "        x = self.pool1(x)           # (batch, 20, 10, 10)\n",
                "        \n",
                "        # Second conv block\n",
                "        x = self.conv2(x)           # (batch, 50, 10, 10)\n",
                "        x = self.relu(x)\n",
                "        x = self.pool2(x)           # (batch, 50, 5, 5)\n",
                "        \n",
                "        # Flatten\n",
                "        x = x.view(x.size(0), -1)   # (batch, 1250)\n",
                "        \n",
                "        # Fully connected layers\n",
                "        x = self.fc1(x)             # (batch, 500)\n",
                "        x = self.relu(x)\n",
                "        x = self.fc2(x)             # (batch, n_classes)\n",
                "        \n",
                "        # Note: No softmax here - CrossEntropyLoss includes it\n",
                "        return x\n",
                "\n",
                "# Create model\n",
                "model = CaptchaCNN(n_classes).to(device)\n",
                "\n",
                "# Print model summary\n",
                "print(model)\n",
                "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n",
                "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "N_EPOCHS = 10\n",
                "LEARNING_RATE = 0.001\n",
                "\n",
                "# Loss function and optimizer\n",
                "# CrossEntropyLoss combines LogSoftmax and NLLLoss\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "\n",
                "# Path of model weights file\n",
                "MODEL_WEIGHTS_PATH = \"./captcha-model-pytorch.pth\"\n",
                "# Force training even if weights are already available\n",
                "FORCE_TRAINING = True\n",
                "\n",
                "# Training history\n",
                "history = {\n",
                "    'train_loss': [],\n",
                "    'train_acc': [],\n",
                "    'val_loss': [],\n",
                "    'val_acc': []\n",
                "}\n",
                "\n",
                "def train_epoch(model, loader, criterion, optimizer, device):\n",
                "    \"\"\"Train for one epoch.\"\"\"\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for inputs, labels in loader:\n",
                "        inputs, labels = inputs.to(device), labels.to(device)\n",
                "        \n",
                "        # Zero gradients\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Forward pass\n",
                "        outputs = model(inputs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        \n",
                "        # Backward pass\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        # Statistics\n",
                "        running_loss += loss.item() * inputs.size(0)\n",
                "        _, predicted = torch.max(outputs, 1)\n",
                "        total += labels.size(0)\n",
                "        correct += (predicted == labels).sum().item()\n",
                "    \n",
                "    epoch_loss = running_loss / total\n",
                "    epoch_acc = correct / total\n",
                "    return epoch_loss, epoch_acc\n",
                "\n",
                "def validate_epoch(model, loader, criterion, device):\n",
                "    \"\"\"Validate for one epoch.\"\"\"\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for inputs, labels in loader:\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "            \n",
                "            outputs = model(inputs)\n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            running_loss += loss.item() * inputs.size(0)\n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "    \n",
                "    epoch_loss = running_loss / total\n",
                "    epoch_acc = correct / total\n",
                "    return epoch_loss, epoch_acc\n",
                "\n",
                "if FORCE_TRAINING or not os.path.exists(MODEL_WEIGHTS_PATH):\n",
                "    print(\"Starting training...\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    for epoch in range(N_EPOCHS):\n",
                "        # Train\n",
                "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
                "        # Validate\n",
                "        val_loss, val_acc = validate_epoch(model, vali_loader, criterion, device)\n",
                "        \n",
                "        # Save history\n",
                "        history['train_loss'].append(train_loss)\n",
                "        history['train_acc'].append(train_acc)\n",
                "        history['val_loss'].append(val_loss)\n",
                "        history['val_acc'].append(val_acc)\n",
                "        \n",
                "        # Print progress\n",
                "        print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
                "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
                "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
                "    \n",
                "    print(\"=\" * 60)\n",
                "    print(\"Training complete!\")\n",
                "    \n",
                "    # Save model weights\n",
                "    torch.save(model.state_dict(), MODEL_WEIGHTS_PATH)\n",
                "    print(f\"Model saved to {MODEL_WEIGHTS_PATH}\")\n",
                "else:\n",
                "    # Load model weights\n",
                "    model.load_state_dict(torch.load(MODEL_WEIGHTS_PATH))\n",
                "    print(f\"Model loaded from {MODEL_WEIGHTS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Plot Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if history['train_loss']:  # Only plot if we trained\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # Loss plot\n",
                "    ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
                "    ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
                "    ax1.set_xlabel('Epoch')\n",
                "    ax1.set_ylabel('Loss')\n",
                "    ax1.set_title('Training and Validation Loss')\n",
                "    ax1.legend()\n",
                "    ax1.grid(True)\n",
                "    \n",
                "    # Accuracy plot\n",
                "    ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
                "    ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
                "    ax2.set_xlabel('Epoch')\n",
                "    ax2.set_ylabel('Accuracy')\n",
                "    ax2.set_title('Training and Validation Accuracy')\n",
                "    ax2.legend()\n",
                "    ax2.grid(True)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluation\n",
                "\n",
                "Test the complete CAPTCHA recognition pipeline on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load labels from file\n",
                "with open(LABELS_PATH, \"rb\") as f:\n",
                "    le = pickle.load(f)\n",
                "\n",
                "# Set model to evaluation mode\n",
                "model.eval()\n",
                "\n",
                "# Dummy character images for failed extractions\n",
                "DUMMY_CHAR_IMAGES = np.zeros((4, 20, 20, 1))\n",
                "\n",
                "# Indices of CAPTCHAs on which extractions failed\n",
                "extract_failed_indices = []\n",
                "# Extracted character images\n",
                "char_images_test = []\n",
                "\n",
                "# Extract character images and make features\n",
                "for i, captcha_image in enumerate(captcha_images_test):\n",
                "    # Extract character images\n",
                "    char_images = extract_chars(captcha_image)\n",
                "    \n",
                "    if char_images:\n",
                "        char_images_test.extend(char_images)\n",
                "    else:\n",
                "        # Use dummy character images as placeholder if extraction failed\n",
                "        extract_failed_indices.append(i)\n",
                "        char_images_test.extend(DUMMY_CHAR_IMAGES)\n",
                "\n",
                "print(f\"Failed extractions: {len(extract_failed_indices)}\")\n",
                "\n",
                "# Make features for character images\n",
                "features_test = [make_feature(char_image) for char_image in char_images_test]\n",
                "# Scale raw pixel values into range [0, 1]\n",
                "features_test = np.array(features_test, dtype=\"float\") / 255\n",
                "\n",
                "# Convert to PyTorch tensor (N, C, H, W)\n",
                "features_test_tensor = torch.FloatTensor(features_test).permute(0, 3, 1, 2).to(device)\n",
                "\n",
                "# Predict labels with neural network\n",
                "with torch.no_grad():\n",
                "    outputs = model(features_test_tensor)\n",
                "    _, predicted = torch.max(outputs, 1)\n",
                "    preds_test = predicted.cpu().numpy()\n",
                "\n",
                "# Convert encoded labels back to original\n",
                "preds_test = le.inverse_transform(preds_test)\n",
                "\n",
                "# Group all 4 characters for the same CAPTCHA\n",
                "preds_test = [\"\".join(chars) for chars in group_every(preds_test, 4)]\n",
                "\n",
                "# Update result for CAPTCHAs on which extractions failed\n",
                "for i in extract_failed_indices:\n",
                "    preds_test[i] = \"-\"\n",
                "\n",
                "print(f\"Total predictions: {len(preds_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compute Accuracy and Visualize Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Number of CAPTCHAs to display\n",
                "N_DISPLAY_SAMPLES = 10\n",
                "\n",
                "# Number of test CAPTCHAs\n",
                "n_test = len(captcha_texts_test)\n",
                "# Number of correct predictions\n",
                "n_correct = 0\n",
                "\n",
                "# Indices of correct predictions\n",
                "correct_indices = []\n",
                "# Indices of incorrect predictions\n",
                "incorrect_indices = []\n",
                "\n",
                "for i, (pred_text, actual_text) in enumerate(zip(preds_test, captcha_texts_test)):\n",
                "    if pred_text == actual_text:\n",
                "        # 1) Update number of correct predictions\n",
                "        n_correct += 1\n",
                "        # 2) Collect index of correct prediction\n",
                "        if len(correct_indices) < N_DISPLAY_SAMPLES:\n",
                "            correct_indices.append(i)\n",
                "    else:\n",
                "        # 3) Collect index of incorrect prediction\n",
                "        if len(incorrect_indices) < N_DISPLAY_SAMPLES:\n",
                "            incorrect_indices.append(i)\n",
                "\n",
                "# Show number of total / correct predictions and accuracy\n",
                "print(\"=\" * 50)\n",
                "print(\"EVALUATION RESULTS (PyTorch)\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"# of test CAPTCHAs: {n_test}\")\n",
                "print(f\"# correctly recognized: {n_correct}\")\n",
                "print(f\"Accuracy: {n_correct/n_test:.4f} ({n_correct/n_test*100:.2f}%)\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show correct predictions\n",
                "print(\"\\n✅ CORRECT PREDICTIONS:\")\n",
                "print_images(\n",
                "    [captcha_images_test[i] for i in correct_indices],\n",
                "    texts=[f\"Correct: {captcha_texts_test[i]}\" for i in correct_indices],\n",
                "    n_rows=2\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show incorrect predictions\n",
                "print(\"\\n❌ INCORRECT PREDICTIONS:\")\n",
                "print_images(\n",
                "    [captcha_images_test[i] for i in incorrect_indices],\n",
                "    texts=[\n",
                "        f\"Pred: {preds_test[i]}\\nActual: {captcha_texts_test[i]}\"\n",
                "        for i in incorrect_indices\n",
                "    ],\n",
                "    n_rows=2,\n",
                "    fig_size=(20, 6),\n",
                "    text_center=(0.5, -0.25)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary: TensorFlow vs PyTorch Comparison\n",
                "\n",
                "| Aspect | TensorFlow/Keras | PyTorch |\n",
                "|--------|-----------------|--------|\n",
                "| Model Definition | `Sequential()` with `.add()` | `nn.Module` class with `__init__` and `forward` |\n",
                "| Data Format | (N, H, W, C) | (N, C, H, W) |\n",
                "| Training Loop | `model.fit()` | Manual loop with `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()` |\n",
                "| Loss Function | Separate softmax + categorical_crossentropy | `CrossEntropyLoss` (includes softmax) |\n",
                "| GPU Usage | Automatic | Manual with `.to(device)` |\n",
                "| Model Saving | `.save_weights()` | `torch.save(model.state_dict())` |\n",
                "\n",
                "Both frameworks should achieve similar accuracy on this CAPTCHA recognition task!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "1. How to break a CAPTCHA system in 15 minutes with Machine Learning: https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710\n",
                "2. PyTorch Documentation: https://pytorch.org/docs/stable/index.html\n",
                "3. PyTorch nn.Module: https://pytorch.org/docs/stable/nn.html\n",
                "4. PyTorch Conv2d: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}